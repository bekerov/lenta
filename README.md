# Анализируем Lenta.ru
### What, How, Why

На мой взгляд, применяя вопросы "What, How, Why" в отношении какой-либо задачи, самое важное будет "Why". Именно ответ на этот вопрос определит что в итоге добьетесь и как вы это сделаете. В моем случае коротким ответом на вопрос "почему" будет "получение опыта". Более развернутым объяснением будет выполнение какого-либо реального задания, в рамках которого я смогу применить навыки, полученные во время обучения, а так же получить результат, который я смог показывать в качестве подтверждения своих умений. Хотя на самом деле вся интрига "а зачем тебе это все" раскрыта  [вот здесь](https://habrahabr.ru/post/329906). 

В качестве объекта исследования была выбрана [Lenta.ru](https://lenta.ru). От части, потому что я являюсь ее давним читателем, хоть и регулярно плююсь от того шлака, который проскакивает мимо редакторов (если таковые там вообще имеются). От части, потому что она показалась относительно легким для data mining. Однако если быть честным, то подходя к выбору объекта я практически не учитывал вопросы "а что я буду с этой датой делать" и "какие вопросы буду задавать". И связано это с тем, что на текущий момент я более менее освоил только Getting and Cleaning Data и мои знания в части анализа очень скудны. Я конечно представлял себе, что как минимум могу ответить на вопрос "как изменилась среднеедневное количество публикуемых новостей за последние 5-10 лет", но дальше этого я не задумаывался.  

И так, эта статья будет посвящена по большей части добыванию и очистке данных, которые будут пригодны для анализа. 

### Grabbing
Первым делом мне необходимо было определиться, как сграббить и распарсить содержимое страниц ресурса. Google подсказал, что оптимальным для этого будет использование пакета [rvest](https://cran.r-project.org/web/packages/rvest/rvest.pdf), который одновременно позволяет получить текст страницы по ее адресу и при помощи xPath выдергивать содержимое нужных мне полей. Конечно, продвинувшись дальше, мне пришлось разбить задачу получения страниц и непосредственный парсинг на две, но это я понял позже, а пока первым шагом было получение списка ссылок на сами статьи.

После недолгого изучения, на сайте был обнаружен раздел "Архив", который при помощи простого скрипта переадресовывал меня на страницу, содержащую ссылки все новости за определенную дату и путь к этой странице выглядел как https://lenta.ru/2017/07/01/ или https://lenta.ru/2017/03/09/. Оставалось только пройтись по всем этим страницам и получить эти самые ссылки. В чем мне и помог этот нехитрый код:
```R
  # prepare vector of links of archive pages in https://lenta.ru//yyyy/mm/dd/ format
  dayArray <- seq(as.Date(articlesStartDate), as.Date(articlesEndDate), 
                  by="days")
  archivePagesLinks <- paste0(baseURL, "/", year(dayArray), 
                      "/", formatC(month(dayArray), width = 2, format = "d", flag = "0"), 
                      "/", formatC(day(dayArray), width = 2, format = "d", flag = "0"), 
                      "/")
  articlesLinks <- c()
  for (i in 1:length(archivePagesLinks)) {
    pg <- read_html(archivePagesLinks[i], encoding = "UTF-8")
    total <- html_nodes(pg, 
              xpath=".//section[@class='b-longgrid-column']//div[@class='titles']//a") %>% 
              html_attr("href")   
    articlesLinks <- c(articlesLinks, total)
    saveRDS(articlesLinks, file.path(tempDataFolder, "tempArticlesLinks.rds"))
  }
  articlesLinks <- paste0(baseURL, articlesLinks)
  writeLines(articlesLinks, file.path(tempDataFolder, "articles.urls"))
```
Сгенерировав массив дат с "2010-01-01" по "2017-06-30" и преобразовав `archivePagesLinks`, я получил ссылки на все так называемые "архивные страницы":

```
> head(archivePagesLinks)
[1] "https://lenta.ru/2010/01/01/"
[2] "https://lenta.ru/2010/01/02/"
[3] "https://lenta.ru/2010/01/03/"
[4] "https://lenta.ru/2010/01/04/"
[5] "https://lenta.ru/2010/01/05/"
[6] "https://lenta.ru/2010/01/06/"
> length(archivePagesLinks)
[1] 2738
```
При помощи метода `read_html` я "скачал" содержимое страниц в буфер, а при помощи методов `html_nodes` и `html_attr` получил непосредственно ссылки на статьи:
```
> head(articlesLinks)
[1] "https://lenta.ru/news/2009/12/31/kids/"     
[2] "https://lenta.ru/news/2009/12/31/silvio/"   
[3] "https://lenta.ru/news/2009/12/31/postpone/" 
[4] "https://lenta.ru/photo/2009/12/31/meeting/" 
[5] "https://lenta.ru/news/2009/12/31/boeviks/"  
[6] "https://lenta.ru/news/2010/01/01/celebrate/"
> length(articlesLinks)
[1] 379862
```

После получения первых результатов я осознал первую проблему. Код приведенный выше выполнялся примерно 40 мин. С учетом размера массива в 2738 ссылок, который был обработан за это время, можно посчитать, что для обработки 379862 ссылок уйдет 5550 минут. Встроенные методы `readLines {base}` и `download.file {utils}`, которые позволяли просто получить текст, давали схожие результаты. Метод `htmlParse {XML}`, который позволял аналогично `read_html` продолжить парсинг содержимого, также ситуацию не улучшил. Тот же результат с испозованием `getURL {RCurl}`. Тупик.

В поисках решения проблемы, я решил посмотреть в сторону "параллельного" исполнения моих запросов, так в момент работы кода ни сеть, ни память, ни процессор не были загружены. Гугл подсказал посмотреть в сторону `parallel-package {parallel}`. Несколько часов изучения и тестов показали, что профита с запуском даже в двух "параллельных" потоках почему нет. Обрывочные сведения в гугле рассказали, что с помощью этого пакета можно распараллеть какие-нибудь вычисления или манипуляции с данными, но при работе с диском или внешним источником все запросы выполняются в рамках одного процесса и выстраиваются в очередь. Да и как понял, ожидать увеличение производительности стоило только кратно имеющимся ядрам, что даже при наличии 8 штук предвещало 690 минут. Возможно я просто не совсем освоил "как это готовить", но я снова оказался в тупике.

Когда гугл перестал мне выдавать новые результаты, я с чистой совестью решил обратиться за помощью к залу. Так как гугл довольно часто выдавал [stackoverflow](https://stackoverflow.com), я решил попытать счастье именно там. Имея опыт общения на тематических форумах и зная реакцию на вопросы новичков, попытался максимально четко и ясно изложить [проблему](https://stackoverflow.com/questions/39180106/i-have-to-grab-plantext-from-over-290k-webpages-is-there-a-way-to-improve-the-s). И о чудо, спустя какие несколько часов я получил от [Bob Rudis](https://rud.is/) более чем развернутый ответ, который после подстановки в мой код, практически полностью решал мою задачу. Правда с оговоркой: я совершенно не понимал как он работает. Я первый раз слышал про `wget`, что в коде делают с `WARC` и зачем в метод передают функцию. Однако если долго-долго смотреть на код, пробовать выполнять его по кускам, разбирая функцией за функцией, можно добиться определенных результатов. С `wget` мне помог справиться все тот же гугл. 

В итоге суть решения свелась к следующему - предварительно подготовленный файл, содержащий ссылки на статьи, подсовывался команде `wget`:
```
  wget --warc-file=lenta -i lenta.urls
```
Непосредственно код выполнения выглядел так:
```R
  system("wget --warc-file=lenta -i lenta.urls", intern = FALSE)
```
После выполнения, я получал кучу файлов (по одному на каждую переданную ссылку) с html содержимым веб-страниц. Также в моем распоряжении был запакованный `WARC`, который содержал в себе лог общения с ресурсом, а так же то самое содержимое веб-страниц. Именно `WARC` и предлагал парсить [Bob Rudis](https://rud.is/). 

Первые замеры производительности показали, что для закачки 2000 ссылок было потрачено 10 минут, методом экстраполяции (давно хотел использовать это слово) получал 1890 минут для всех статей - "олмост три таймс фастер бат нот энаф" - почти в три раза быстрее, но все равно недостаточно.

Вернувшись на пару шагов назад и протестив новый механизм

